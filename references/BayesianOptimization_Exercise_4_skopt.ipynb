{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "Q1wnryKmxkOz",
    "outputId": "f719230f-6b4c-4861-e404-884ecc7d57ec"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms, utils\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterSampler, RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "import random\n",
    "np.random.seed(32)\n",
    "random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6ZbPxoDxkPB"
   },
   "source": [
    "# Hyperparameters tuning using Bayesian Optimization with the skopt library\n",
    "\n",
    "As you are going to see, the project for this first part of the course will be to use Bayesian Optimization to tune the hyperparameters of a Random forest classifier. To do that you are going to use a library that is more optimized compare to the code from scratch that we have written before.\n",
    "\n",
    "In this exercise you are going to use Bayesian Optimization to select the best hyperparameters of a random forest trained on part of the MNIST dataset. You are going also to compare it with respect to a random search on the hyperparameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uup6huN4xkPJ"
   },
   "outputs": [],
   "source": [
    "def load_MNIST():\n",
    "    '''\n",
    "    Function to load the MNIST training and test set with corresponding labels.\n",
    "\n",
    "    :return: training_examples, training_labels, test_examples, test_labels\n",
    "    '''\n",
    "\n",
    "    # we want to flat the examples\n",
    "\n",
    "    training_set = datasets.MNIST(root='./data', train=True, download=True, transform= None)\n",
    "    test_set = datasets.MNIST(root='./data', train=False, download=True, transform= None)\n",
    "\n",
    "    Xtrain = training_set.data.numpy().reshape(-1,28*28)\n",
    "    Xtest = test_set.data.numpy().reshape(-1,28*28)\n",
    "\n",
    "    ytrain = training_set.targets.numpy()\n",
    "    ytest = test_set.targets.numpy()\n",
    "\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIx5w0wjxkPR"
   },
   "outputs": [],
   "source": [
    "## since training a random forest on the entire dataset takes some time\n",
    "## we can consider only few labels, like 3, 5, 8 and 9\n",
    "\n",
    "## we can load the training set and test set\n",
    "Xtrain, ytrain, Xtest, ytest = load_MNIST()\n",
    "\n",
    "# Limit the number of training examples mainly just for speed\n",
    "Xtrain=Xtrain[:1000]\n",
    "ytrain=ytrain[:1000]\n",
    "\n",
    "\n",
    "# print some information\n",
    "print('Information about the new datasets')\n",
    "print('Training set shape:', Xtrain.shape)\n",
    "print('Test set shape', Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLelVc36xkPa"
   },
   "source": [
    "## Random forest\n",
    "\n",
    "As you have seen in your Machine Learning course, there are a lot of hyperparameters to choose before training a Random Forest. If you look at the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) you can notice all the paramters. For this exercise we will focus only on the following four hyperparameters:\n",
    "\n",
    "1. **n_estimators**: the number of decision trees that are in the forest;\n",
    "2. **criterion**: the criterion to evaluate the split. We should decide between *Gini impurity* (`gini`) and *information gain* (`entropy`);\n",
    "3. **max_depth**: Maximum depth of the trees. If None, the tree expands until we have a element in all the leaves or  until all leaves contain less than min_samples_split samples.\n",
    "4. **max_features**: The number of features to consider when looking for the best split. We have to choose among `'sqrt'` where we consider `sqrt(n_features)`, `'log2'` where we consider `log2(n_features)` or None which will just use all features.\n",
    "\n",
    "Remember that when we are using Random Forest we can avoid running the cross-validation to get the validation error as approximation of the test error, but instead we can use the *out of bag* error to get an approximation of the test error we are going to get when we consider unseen example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKz2Nq5OxkPc"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(oob_score=True,n_estimators=10,criterion='gini',max_depth=10,max_features='sqrt')\n",
    "model.fit(Xtrain, ytrain)\n",
    "print('Default model out of bag error', model.oob_score_)\n",
    "print('Default model test accuracy', model.score(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2DIwTl5xkPk"
   },
   "source": [
    "### Random search \n",
    "\n",
    "A possible way to find the best hyperparameters is to use random search over the parameter space. To perform this operation we can use `RandomizedSearchCV`. You can look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). However, since we do not need cross-validation for Random Forests, we can use `ParameterSampler` to sample random parameters from our parameter space. Look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html#sklearn.model_selection.ParameterSampler) To be able to use this method you should first define a dictionary of the hyperparameters you want to optimize, in this case the four we mentioned above, and then decide a way to evaluate the model and how many folds for the cross-validation. The dictionary is of the form:\n",
    "\n",
    "```python\n",
    "params = {\"name_params\": uniform(0, 1), # if it is continuous between 0 and 1\n",
    "           \"name_params2\": range(1,50), # if it is discrete but integer\n",
    "           \"name_params3\": ['name1', 'name2']} # if it is discrete but string`\n",
    "```\n",
    "\n",
    "<font color='blue'> Tasks:\n",
    "\n",
    "1. <font color='blue'> Crete the dictionary of the hyperparameters, we recommend keeping the value of the number of trees up to 150 otherwise it takes a very long time and the max_depth up to 100. After that, you can use the dictionary to create a random hyperparameter using `ParameterSampler`. The `ParameterSampler` call is already provided.\n",
    "\n",
    "2. <font color='blue'> Using the parameters returned by the `ParameterSampler` you should fit a random forest, and for each iteration you should store the best value of the `model.oob_score_` you get. **HINT:** You can access the hyperparameters value by the name you used in the defined dictionary. (The whole process should take up to a few minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQP9z8AOxkPm"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "# hyperparams dictionary \n",
    "domain = \n",
    "\n",
    "# create the ParameterSampler\n",
    "param_list = list(ParameterSampler(domain, n_iter=20, random_state=32))\n",
    "print('Param list')\n",
    "print(param_list)\n",
    "\n",
    "## now we can train the random forest using these parameters tuple, and for\n",
    "## each iteration we store the best value of the oob\n",
    "\n",
    "current_best_oob = 0\n",
    "iteration_best_oob = 0 \n",
    "max_oob_per_iteration = []\n",
    "i = 0\n",
    "for params in param_list:\n",
    "    print(i)\n",
    "    print(params)\n",
    "    \n",
    "    #define model here\n",
    "    \n",
    "    start = time.time()\n",
    "    #train the model\n",
    "\n",
    "    end = time.time()\n",
    "    # extract oob_score and update current_best_oob if better that the current best\n",
    "    \n",
    "    max_oob_per_iteration.append(current_best_oob)\n",
    "    i += 1\n",
    "    print(f'It took {end - start} seconds')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NmwnVJ2xkPt"
   },
   "source": [
    "### Bayesian Optimization\n",
    "\n",
    "The procedure we are interested in, instead, is Bayesian Optimization. Here we will use the scikit-optimize (skopt) package and the gp_minimize function.\n",
    "For this we will meed to define an objective function and define the bounds to optimize within.\n",
    "<br>Unfortunately, skopt does not allow None to be passed as a categorical option is is actually used here, therefore you have to handle the None option for the max_features by converting the input from a string to None inside the objective function.\n",
    "<br>skopt will use one-out-of-k encoding for categorical variables with more than 2 options\n",
    "\n",
    "In case of random forest, you should return the `oob_score` and not the validation score. If you are going to use other models you should consider the validation set, instead.\n",
    "\n",
    "You should create an Bayesian Optimization instancer using the following function `skopt.gp_minimize`, see the complete description [here](https://scikit-optimize.github.io/stable/modules/generated/skopt.gp_minimize.html).\n",
    "\n",
    "At the end of the optimization, you can access the array of the best parameters by `opt.x`. You can also collect the best `oob_score` per iteration using \n",
    "`np.maximum.accumulate(-opt.func_vals).ravel()`.\n",
    "\n",
    "Investigate the documentation for gp_minimize, use the EI acqusition function and set the xi parameter to 0.1 and the noise to 0.01**2.\n",
    "\n",
    "\n",
    "<font color='blue'> Tasks:\n",
    "\n",
    "1. <font color='blue'>You should define an objective function which can be called and returns the negative oob_score.\n",
    "\n",
    "2. <font color='blue'>In the same plot, show the best `oob_score` per iteration you obtain using the random search and the bayesian optimization. What do you see?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2TTDArnxkPv"
   },
   "outputs": [],
   "source": [
    "import skopt\n",
    "from skopt import gp_minimize\n",
    "\n",
    "#start at same initial point\n",
    "x0=[param_list[0]['n_estimators'],param_list[0]['max_depth'],param_list[0]['max_features'],param_list[0]['criterion']]\n",
    "if x0[2] is None:\n",
    "    x0[2] = 'None'\n",
    "\n",
    "\n",
    "y0 = -max_oob_per_iteration[0]\n",
    "\n",
    "\n",
    "## define the domain of the considered parameters\n",
    "n_estimators = (1,150)\n",
    "max_depth=(1,100)\n",
    "max_features = ('log2', 'sqrt', 'None')\n",
    "criterion = ('gini', 'entropy')\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "global i\n",
    "i = 1\n",
    "def objective_function(x): \n",
    "    if x[2]=='None':\n",
    "        maxf = None\n",
    "    else:\n",
    "        maxf = x[2]\n",
    "    \n",
    "    #create the model\n",
    "    \n",
    "    # fit the model \n",
    "    \n",
    "    global i\n",
    "    i += 1\n",
    "    print(i)\n",
    "    print(x)\n",
    "    print(model.oob_score_)\n",
    "    \n",
    "    return - model.oob_score_\n",
    "\n",
    "np.int = int #numpy np.int deprecation workaround\n",
    "opt = gp_minimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DvhNlzcxkP2"
   },
   "outputs": [],
   "source": [
    "## comparison between random search and bayesian optimization\n",
    "## we can plot the maximum oob per iteration of the sequence\n",
    "\n",
    "# collect the maximum each iteration of BO\n",
    "\n",
    "# define iteration number\n",
    "xs = np.arange(1,21,1)\n",
    "\n",
    "plt.plot(xs, max_oob_per_iteration, 'o-', color = 'red', label='Random Search')\n",
    "plt.plot(xs, y_bo, 'o-', color = 'blue', label='Bayesian Optimization')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Out of bag error')\n",
    "plt.title('Comparison between Random Search and Bayesian Optimization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6dHDP0rU-5r"
   },
   "source": [
    "# [OPTIONAL] Investigate the acquisition function\n",
    "HINTS:<br> Important: note that skopt normalizes the input space between 0 and 1, hence you have to convert the values to evaluate to this interval<br>\n",
    "As the function is of more than 2 dimensional in this case it is not trivial to plot it, therefore keep the categorical variables fixed at some value (for example at zero).<br>Note that skopt actually just uses a normal GP even for the categorical variables in this case, try to thihnk about why this is probably not a good approach.<br> Then you can evaluate the acquisition function on a grid for the remaining two variables using np.meshgrid.\n",
    "<br>\n",
    "<font color='blue'>\n",
    "1. <font color='blue'> Investigate the dimensionality of the GP (look at the model subclass), how many variables are there?<br>\n",
    "2. <font color='blue'> Plot the surrogate function and its standard deviation, you can use the models predict function <br>\n",
    "3. <font color='blue'> Take a look at the acqusition function.  The acquisition function can be evaluated using <a href=https://scikit-optimize.github.io/stable/modules/generated/skopt.acquisition.gaussian_ei.html#skopt.acquisition.gaussian_ei>skopt.acquisition.gaussian_ei</a>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the line below shows how to encode 'log2' as max_features\n",
    "print(opt.space[2][1].transform(['log2']))\n",
    "# the line below shows how to encode the criterion 'gini'\n",
    "print(opt.space[3][1].transform(['gini']))\n",
    "\n",
    "n_estimators = (1,151) #add 1 to include limit\n",
    "max_depth=(1,101) #add 1 to include the limit\n",
    "\n",
    "\n",
    "#We start by taking a look a the model subclass of our Bayesian optimization object\n",
    "print(opt)\n",
    "#get the last model\n",
    "model = opt.models[-1]\n",
    "#To get a plot of the acquisition function we use the model's predict function\n",
    "#first we define a sensible grid for the first to parameters\n",
    "#indexing='ij' ensures that x/y axes are not flipped (which is default):\n",
    "#note that the acqusition function can actually take any value not only integers as it lives in the GP space\n",
    "#and it is quite fast to evaluate even for many points\n",
    "\n",
    "# this code will create an array of points to evaluate the function in the normalized space\n",
    "# the result should be a 6xNpoints array where the first two rows are the n_estimators and max_depth\n",
    "# the last 4 column are for the categorical variables, to encode 'log2' the first column is set to 1\n",
    "# the last column will indicate 'gini' as the first option for the criterion\n",
    "pgrid = np.array(np.meshgrid(np.arange(*n_estimators), np.arange(*max_depth),indexing='ij'))\n",
    "pgrid_norm = np.array([opt.space[i][1].transform(x) for i,x in enumerate(pgrid.reshape(2,-1))])\n",
    "extra_dims = np.zeros((4,pgrid_norm.shape[1]))\n",
    "extra_dims[0,:] = 1\n",
    "pgrid_norm = np.concatenate((pgrid_norm,extra_dims),axis=0)\n",
    "print(pgrid_norm.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to evaluate the surrogate function and it's standard deviation in the normalized grid\n",
    "# then reshape into image and plot using the plt.imshow function (already provided)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(ye_img.T, origin='lower',extent=[pgrid[0][0][0],pgrid[0][-1][0],pgrid[0][0][0],pgrid[1][0][-1]])\n",
    "plt.colorbar()\n",
    "plt.xlabel('n_features')\n",
    "plt.ylabel('max_depth')\n",
    "plt.title('Surrogate function value')\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(ye_std_img.T, origin='lower',extent=[pgrid[0][0][0],pgrid[0][-1][0],pgrid[1][0][0],pgrid[1][0][-1]])\n",
    "plt.colorbar()\n",
    "plt.xlabel('n_features')\n",
    "plt.ylabel('max_depth')\n",
    "plt.title('Surrogate function std dev');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the acqusition function using skopt.acquisition.gaussian_ei and plot it\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(acq_val_img.T, origin='lower',extent=[pgrid[0][0][0],pgrid[0][-1][0],pgrid[0][0][0],pgrid[1][0][-1]])\n",
    "plt.colorbar()\n",
    "plt.xlabel('n_features')\n",
    "plt.ylabel('max_depth')\n",
    "plt.title('Acquisition function');"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture_2_Exercise2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
